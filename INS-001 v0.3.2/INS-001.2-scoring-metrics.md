# INS-001.2 Scoring Metrics Specification

## Overview

INS-001.2 (Semantic Associations) uses exactly **two metrics** to evaluate participant-submitted concept unions:

1. **Relevance** — Is this clue in the semantic neighborhood of the anchor-target pair?
2. **Divergence** — How far off-axis is the clue from the direct anchor-target path?

These metrics are orthogonal:
- High relevance + high divergence = creative but valid
- High relevance + low divergence = predictable/conventional  
- Low relevance = noise (divergence becomes meaningless)

---

## Metric 1: Relevance

**Purpose:** Distinguish signal from noise. A clue is relevant if it's semantically connected to both the anchor and target concepts.

**Formula:** Mean cosine similarity to both endpoints.

```python
def calculate_relevance(
    clue_embedding: list[float],
    anchor_embedding: list[float],
    target_embedding: list[float]
) -> float:
    """
    How semantically connected is this clue to the anchor-target pair?
    
    Args:
        clue_embedding: Embedding vector for the clue
        anchor_embedding: Embedding vector for anchor concept
        target_embedding: Embedding vector for target concept
        
    Returns:
        Relevance score in range [0, 1]
        Higher = more connected to both endpoints
    """
    sim_anchor = cosine_similarity(clue_embedding, anchor_embedding)
    sim_target = cosine_similarity(clue_embedding, target_embedding)
    return (sim_anchor + sim_target) / 2


def calculate_relevance_batch(
    clue_embeddings: list[list[float]],
    anchor_embedding: list[float],
    target_embedding: list[float]
) -> dict:
    """
    Calculate relevance for a set of clues.
    
    Returns:
        Dictionary with:
        - overall: Mean relevance across all clues
        - individual: List of relevance scores per clue
    """
    individual = [
        calculate_relevance(clue, anchor_embedding, target_embedding)
        for clue in clue_embeddings
    ]
    return {
        "overall": float(np.mean(individual)) if individual else 0.0,
        "individual": individual
    }
```

**Interpretation thresholds** (calibrate empirically):

| Score | Interpretation |
|-------|----------------|
| < 0.15 | Noise / unrelated |
| 0.15–0.30 | Weak (tangential) |
| 0.30–0.45 | Moderate (connected) |
| > 0.45 | Strong (core neighborhood) |

**Literature basis:**
- Standard in information retrieval: query-document relevance with multi-term queries uses additive/mean combination
- Distributional semantics: semantic neighborhood defined by average proximity to reference points

---

## Metric 2: Divergence

**Purpose:** Measure creativity. How far does the associative path arc away from the direct route between anchor and target?

**Formula:** Angular deviation from the anchor-target axis, measured from the midpoint.

```python
def calculate_divergence(
    clue_embedding: list[float],
    anchor_embedding: list[float],
    target_embedding: list[float]
) -> float:
    """
    Angular divergence: how far off-axis is this clue from the A-T line?
    
    Args:
        clue_embedding: Embedding vector for the clue
        anchor_embedding: Embedding vector for anchor concept
        target_embedding: Embedding vector for target concept
        
    Returns:
        Divergence in degrees (0-90 scale)
        0 = clue lies exactly on the A-T axis
        90 = clue is perpendicular to the axis
    """
    anchor_vec = np.array(anchor_embedding)
    target_vec = np.array(target_embedding)
    clue_vec = np.array(clue_embedding)
    
    # A-T axis direction
    axis = target_vec - anchor_vec
    
    # Midpoint of A-T segment
    midpoint = (anchor_vec + target_vec) / 2
    
    # Vector from midpoint to clue
    clue_direction = clue_vec - midpoint
    
    # Handle edge case: clue is exactly at midpoint
    clue_norm = np.linalg.norm(clue_direction)
    if clue_norm < 1e-10:
        return 0.0  # On the axis by definition
    
    axis_norm = np.linalg.norm(axis)
    if axis_norm < 1e-10:
        return 0.0  # Anchor and target are identical
    
    # Cosine of angle between clue_direction and axis
    cos_angle = np.dot(clue_direction, axis) / (clue_norm * axis_norm)
    
    # Clamp for numerical stability
    cos_angle = np.clip(cos_angle, -1.0, 1.0)
    
    # Convert to angle (0-180), then fold to 0-90
    # (we don't care which "side" of the axis, just how far off)
    angle_rad = np.arccos(np.abs(cos_angle))
    angle_deg = np.degrees(angle_rad)
    
    return float(angle_deg)


def calculate_divergence_batch(
    clue_embeddings: list[list[float]],
    anchor_embedding: list[float],
    target_embedding: list[float]
) -> dict:
    """
    Calculate divergence for a set of clues.
    
    Returns:
        Dictionary with:
        - overall: Mean divergence across all clues (degrees)
        - individual: List of divergence scores per clue
    """
    individual = [
        calculate_divergence(clue, anchor_embedding, target_embedding)
        for clue in clue_embeddings
    ]
    return {
        "overall": float(np.mean(individual)) if individual else 0.0,
        "individual": individual
    }
```

**Geometric intuition:**

```
                 clue •
                     /
                    / angle = divergence  
                   /
                  /
anchor •--------M--------• target
              midpoint
```

**Interpretation thresholds:**

| Angle | Interpretation |
|-------|----------------|
| 0–15° | On-axis (direct/predictable path) |
| 15–45° | Moderate arc |
| 45–75° | Strong arc (creative) |
| 75–90° | Perpendicular (highly unconventional) |

**Literature basis:**
- Remote Associates Test (Mednick, 1962): creativity measured by ability to find distant semantic connections
- Semantic distance in associative networks (De Deyne et al., 2019): associative path length predicts creativity judgments

---

## Combined Scoring Function

```python
import numpy as np


def cosine_similarity(a: list[float], b: list[float]) -> float:
    """Cosine similarity between two vectors."""
    a = np.array(a)
    b = np.array(b)
    
    dot = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    
    if norm_a == 0 or norm_b == 0:
        return 0.0
    
    return float(dot / (norm_a * norm_b))


def score_union(
    clue_embeddings: list[list[float]],
    anchor_embedding: list[float],
    target_embedding: list[float]
) -> dict:
    """
    Score a participant's semantic union submission.
    
    Args:
        clue_embeddings: List of embedding vectors for submitted clues
        anchor_embedding: Embedding vector for anchor concept
        target_embedding: Embedding vector for target concept
        
    Returns:
        Dictionary with:
        - relevance: Overall relevance score (0-1)
        - relevance_individual: Per-clue relevance scores
        - divergence: Overall divergence in degrees (0-90)
        - divergence_individual: Per-clue divergence scores
        - valid: Whether submission passes relevance threshold
    """
    if not clue_embeddings:
        return {
            "relevance": 0.0,
            "relevance_individual": [],
            "divergence": 0.0,
            "divergence_individual": [],
            "valid": False
        }
    
    relevance_scores = []
    divergence_scores = []
    
    anchor_vec = np.array(anchor_embedding)
    target_vec = np.array(target_embedding)
    axis = target_vec - anchor_vec
    midpoint = (anchor_vec + target_vec) / 2
    axis_norm = np.linalg.norm(axis)
    
    for clue_emb in clue_embeddings:
        clue_vec = np.array(clue_emb)
        
        # Relevance
        sim_anchor = cosine_similarity(clue_emb, anchor_embedding)
        sim_target = cosine_similarity(clue_emb, target_embedding)
        relevance = (sim_anchor + sim_target) / 2
        relevance_scores.append(relevance)
        
        # Divergence
        clue_direction = clue_vec - midpoint
        clue_norm = np.linalg.norm(clue_direction)
        
        if clue_norm < 1e-10 or axis_norm < 1e-10:
            divergence_scores.append(0.0)
        else:
            cos_angle = np.dot(clue_direction, axis) / (clue_norm * axis_norm)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle_deg = np.degrees(np.arccos(np.abs(cos_angle)))
            divergence_scores.append(angle_deg)
    
    overall_relevance = float(np.mean(relevance_scores))
    overall_divergence = float(np.mean(divergence_scores))
    
    # Validity threshold (calibrate empirically)
    RELEVANCE_THRESHOLD = 0.15
    valid = overall_relevance >= RELEVANCE_THRESHOLD
    
    return {
        "relevance": overall_relevance,
        "relevance_individual": relevance_scores,
        "divergence": overall_divergence,
        "divergence_individual": divergence_scores,
        "valid": valid
    }
```

---

## Score Normalization

Raw relevance (0-1) and divergence (0-90°) are difficult to interpret without context. Different anchor-target pairs have different baseline geometries—some pairs are naturally closer in embedding space, making high relevance easier to achieve.

**Solution:** Bootstrap null distributions by sampling random word sets from vocabulary. This gives pair-specific baselines.

### Building Null Distributions

```python
def bootstrap_null_distribution(
    anchor_embedding: list[float],
    target_embedding: list[float],
    vocabulary_embeddings: list[list[float]],
    n_clues: int,
    n_samples: int = 1000,
    seed: int = 42
) -> dict:
    """
    Build null distributions for relevance and divergence by sampling
    random word sets from vocabulary.
    
    Args:
        anchor_embedding: Embedding for anchor concept
        target_embedding: Embedding for target concept
        vocabulary_embeddings: List of embeddings for vocabulary words
        n_clues: Number of clues to sample (match participant's submission size)
        n_samples: Number of bootstrap samples
        seed: Random seed for reproducibility
        
    Returns:
        Dictionary with:
        - relevance_mean: Mean relevance under null
        - relevance_std: Std of relevance under null
        - divergence_mean: Mean divergence under null
        - divergence_std: Std of divergence under null
        - relevance_samples: Raw samples (for percentile calculation)
        - divergence_samples: Raw samples
    """
    rng = np.random.default_rng(seed)
    
    relevance_samples = []
    divergence_samples = []
    
    vocab_array = np.array(vocabulary_embeddings)
    n_vocab = len(vocab_array)
    
    for _ in range(n_samples):
        # Sample n random words
        indices = rng.choice(n_vocab, size=n_clues, replace=False)
        sample_embeddings = [vocab_array[i].tolist() for i in indices]
        
        # Score this random set
        scores = score_union(sample_embeddings, anchor_embedding, target_embedding)
        relevance_samples.append(scores["relevance"])
        divergence_samples.append(scores["divergence"])
    
    return {
        "relevance_mean": float(np.mean(relevance_samples)),
        "relevance_std": float(np.std(relevance_samples)),
        "divergence_mean": float(np.mean(divergence_samples)),
        "divergence_std": float(np.std(divergence_samples)),
        "relevance_samples": relevance_samples,
        "divergence_samples": divergence_samples
    }
```

### Normalizing Scores

```python
def normalize_scores(
    participant_scores: dict,
    null_distribution: dict,
    method: str = "percentile"
) -> dict:
    """
    Normalize participant scores against null distribution.
    
    Args:
        participant_scores: Output from score_union()
        null_distribution: Output from bootstrap_null_distribution()
        method: "percentile" (0-100) or "zscore" (standard deviations from mean)
        
    Returns:
        Dictionary with:
        - relevance_normalized: Normalized relevance score
        - divergence_normalized: Normalized divergence score
        - relevance_raw: Original relevance
        - divergence_raw: Original divergence
    """
    rel_raw = participant_scores["relevance"]
    div_raw = participant_scores["divergence"]
    
    if method == "percentile":
        # Percentile rank: what % of null samples is this score greater than?
        rel_norm = float(np.mean([rel_raw > s for s in null_distribution["relevance_samples"]]) * 100)
        div_norm = float(np.mean([div_raw > s for s in null_distribution["divergence_samples"]]) * 100)
        
    elif method == "zscore":
        # Z-score: how many standard deviations from null mean?
        rel_std = null_distribution["relevance_std"]
        div_std = null_distribution["divergence_std"]
        
        # Guard against zero std (degenerate case)
        if rel_std > 0:
            rel_norm = float((rel_raw - null_distribution["relevance_mean"]) / rel_std)
        else:
            rel_norm = 0.0
            
        if div_std > 0:
            div_norm = float((div_raw - null_distribution["divergence_mean"]) / div_std)
        else:
            div_norm = 0.0
        
    else:
        raise ValueError(f"Unknown method: {method}")
    
    return {
        "relevance_normalized": rel_norm,
        "divergence_normalized": div_norm,
        "relevance_raw": rel_raw,
        "divergence_raw": div_raw
    }
```

### Interpretation

**Percentile method (recommended for user-facing display):**

| Percentile | Interpretation |
|------------|----------------|
| < 25th | Below average (worse than random) |
| 25th–50th | Low average |
| 50th–75th | Above average |
| 75th–90th | Good (better than most random sets) |
| 90th–99th | Excellent |
| > 99th | Exceptional |

**Z-score method (recommended for statistical analysis):**

| Z-score | Interpretation |
|---------|----------------|
| < 0 | Below null mean |
| 0–1 | Slightly above average |
| 1–2 | Notably above average |
| > 2 | Significantly above average (p < 0.05) |
| > 3 | Highly significant (p < 0.001) |

**Example output:**

```
Participant clues: ["flame", "wish", "surprise"]
Anchor: VOLCANO, Target: BIRTHDAY

Raw scores:
  Relevance: 0.38
  Divergence: 34°

Normalized (percentile):
  Relevance: 89th percentile  → "Much more relevant than random"
  Divergence: 67th percentile → "Moderately more creative than random"
```

### Optimization Considerations

Bootstrapping 1000 samples per anchor-target pair is computationally expensive. Consider:

1. **Precompute** — Generate null distributions for all anchor-target pairs in your stimulus set ahead of time. Store results in database.

2. **Cache** — Cache distributions keyed by `(anchor, target, n_clues)` tuple. TTL based on vocabulary changes.

3. **Global null** — Sample random sets across all pairs rather than per-pair. Less accurate but much cheaper. May be acceptable if pairs have similar baseline geometries.

4. **Parametric approximation** — Fit a distribution to the bootstrap samples and store only the parameters:
   - Relevance: Beta distribution (bounded 0-1)
   - Divergence: Could try truncated normal or von Mises (bounded 0-90)
   - Reduces storage from 2000 floats to 4-6 parameters per pair

5. **Reduce sample size** — 500 or even 200 samples may be sufficient for percentile estimation if resolution isn't critical.

6. **Lazy computation** — Only compute null distribution when participant submits, then cache. Cold start for first participant per pair.

---

## Comparing Sets

To compare a participant's union against an LLM-generated union (or any two sets):

```python
def compare_unions(
    participant_scores: dict,
    baseline_scores: dict
) -> dict:
    """
    Compare participant union against a baseline (e.g., LLM).
    
    Args:
        participant_scores: Output from score_union() for participant
        baseline_scores: Output from score_union() for baseline
        
    Returns:
        Dictionary with:
        - relevance_delta: Participant relevance - baseline relevance
        - divergence_delta: Participant divergence - baseline divergence
        - more_creative: Whether participant is more divergent (given valid relevance)
    """
    rel_delta = participant_scores["relevance"] - baseline_scores["relevance"]
    div_delta = participant_scores["divergence"] - baseline_scores["divergence"]
    
    # Only compare creativity if both are valid
    both_valid = participant_scores["valid"] and baseline_scores["valid"]
    more_creative = both_valid and div_delta > 0
    
    return {
        "relevance_delta": rel_delta,
        "divergence_delta": div_delta,
        "more_creative": more_creative
    }
```

---

## Metrics to Deprecate

The following metrics from `scoring.py` and `scoring_bridging.py` should be removed or deprecated:

### From `scoring.py`

| Metric | Reason |
|--------|--------|
| `compute_divergence()` | Replaced by angular divergence; used noise floor centroid approach |
| `compute_convergence()` | Not used in INS-001.2; was for single-word reconstruction |
| `compute_semantic_portability()` | Derived metric requiring network/stranger convergence |
| `compute_consistency()` | Cross-game aggregate; compute from raw divergence if needed |
| `compute_llm_alignment()` | Replaced by direct comparison of divergence scores |
| `classify_archetype()` | Dependent on deprecated convergence metrics |
| `FUZZY_EXACT_MATCH_THRESHOLD` | Not needed without reconstruction |

### From `scoring_bridging.py`

| Metric | Reason |
|--------|--------|
| `calculate_divergence()` | Replaced by angular version |
| `calculate_binding_strength()` | Superseded by relevance (mean, not min) |
| `calculate_bridge_similarity()` | Not needed; compare divergence scores directly |
| `calculate_semantic_distance()` | Trivial transform of cosine similarity |
| `calculate_reconstruction()` | Not used in INS-001.2 |
| `calculate_statistical_baseline()` | Complex; use simple relevance ranking instead |
| `calculate_joint_distance_score()` | Equidistance is weaker criterion than relevance |
| `calculate_union_quality()` | Replaced by relevance |
| `get_divergence_interpretation()` | Update for angular scale (degrees) |
| `get_reconstruction_interpretation()` | Deprecated with reconstruction |
| `get_union_quality_interpretation()` | Deprecated with union quality |
| `find_lexical_union()` / `find_lexical_bridge()` | Keep if needed for LLM baseline generation |
| `CALIBRATION_MAX` | Not needed for angular divergence |
| `JOINT_DISTANCE_MAX_DIFF` | Deprecated with joint distance |

### Keep

| Function | Reason |
|----------|--------|
| `cosine_similarity()` | Core building block |
| `find_lexical_union()` | Useful for generating LLM/embedding baseline |
| `_is_morphological_variant()` | Useful for filtering |
| `_get_word_stem()` | Useful for filtering |

---

## Test Cases

```python
def test_relevance():
    """High similarity to both = high relevance."""
    anchor = [1.0, 0.0, 0.0]
    target = [0.0, 1.0, 0.0]
    
    # Clue between both (45° from each)
    clue_relevant = [0.707, 0.707, 0.0]
    rel = calculate_relevance(clue_relevant, anchor, target)
    assert rel > 0.5, f"Expected high relevance, got {rel}"
    
    # Clue orthogonal to both
    clue_noise = [0.0, 0.0, 1.0]
    rel = calculate_relevance(clue_noise, anchor, target)
    assert rel < 0.1, f"Expected low relevance, got {rel}"


def test_divergence_on_axis():
    """Clue on A-T axis should have ~0 divergence."""
    anchor = [1.0, 0.0, 0.0]
    target = [0.0, 1.0, 0.0]
    
    # Clue exactly at midpoint direction (normalized)
    midpoint_dir = np.array([0.707, 0.707, 0.0])
    div = calculate_divergence(midpoint_dir.tolist(), anchor, target)
    assert div < 5.0, f"Expected near-zero divergence, got {div}"


def test_divergence_perpendicular():
    """Clue perpendicular to A-T axis should have ~90° divergence."""
    anchor = [1.0, 0.0, 0.0]
    target = [0.0, 1.0, 0.0]
    
    # Clue perpendicular to A-T plane
    clue_perp = [0.0, 0.0, 1.0]
    div = calculate_divergence(clue_perp, anchor, target)
    assert div > 80.0, f"Expected high divergence, got {div}"


def test_score_union():
    """Integration test for combined scoring."""
    anchor = [1.0, 0.0, 0.0]
    target = [0.0, 1.0, 0.0]
    
    clues = [
        [0.707, 0.707, 0.0],  # On axis, relevant
        [0.5, 0.5, 0.707],    # Off axis, somewhat relevant
    ]
    
    result = score_union(clues, anchor, target)
    
    assert result["valid"] == True
    assert result["relevance"] > 0.3
    assert len(result["relevance_individual"]) == 2
    assert len(result["divergence_individual"]) == 2


def test_bootstrap_null_distribution():
    """Test null distribution generation."""
    anchor = [1.0, 0.0, 0.0]
    target = [0.0, 1.0, 0.0]
    
    # Create a small fake vocabulary (random unit vectors)
    rng = np.random.default_rng(123)
    vocab = []
    for _ in range(100):
        v = rng.standard_normal(3)
        v = v / np.linalg.norm(v)  # Normalize
        vocab.append(v.tolist())
    
    null_dist = bootstrap_null_distribution(
        anchor, target, vocab,
        n_clues=3,
        n_samples=100,  # Reduced for test speed
        seed=42
    )
    
    assert "relevance_mean" in null_dist
    assert "divergence_mean" in null_dist
    assert len(null_dist["relevance_samples"]) == 100
    assert len(null_dist["divergence_samples"]) == 100
    # Random vectors should have ~0 relevance (orthogonal to both)
    assert null_dist["relevance_mean"] < 0.3


def test_normalize_scores_percentile():
    """Test percentile normalization."""
    # Fake participant scores
    participant_scores = {
        "relevance": 0.5,
        "divergence": 45.0
    }
    
    # Fake null distribution where participant is above average
    null_dist = {
        "relevance_samples": [0.1, 0.2, 0.3, 0.4, 0.45] * 20,  # 100 samples, mostly below 0.5
        "divergence_samples": [30, 35, 40, 42, 44] * 20,  # 100 samples, mostly below 45
        "relevance_mean": 0.29,
        "relevance_std": 0.13,
        "divergence_mean": 38.2,
        "divergence_std": 5.3
    }
    
    result = normalize_scores(participant_scores, null_dist, method="percentile")
    
    assert result["relevance_normalized"] > 80  # Should be high percentile
    assert result["divergence_normalized"] > 80  # Should be high percentile
    assert result["relevance_raw"] == 0.5
    assert result["divergence_raw"] == 45.0


def test_normalize_scores_zscore():
    """Test z-score normalization."""
    participant_scores = {
        "relevance": 0.5,
        "divergence": 45.0
    }
    
    null_dist = {
        "relevance_samples": [],  # Not used for z-score
        "divergence_samples": [],
        "relevance_mean": 0.3,
        "relevance_std": 0.1,
        "divergence_mean": 40.0,
        "divergence_std": 5.0
    }
    
    result = normalize_scores(participant_scores, null_dist, method="zscore")
    
    # (0.5 - 0.3) / 0.1 = 2.0
    assert abs(result["relevance_normalized"] - 2.0) < 0.01
    # (45 - 40) / 5 = 1.0
    assert abs(result["divergence_normalized"] - 1.0) < 0.01


if __name__ == "__main__":
    test_relevance()
    test_divergence_on_axis()
    test_divergence_perpendicular()
    test_score_union()
    test_bootstrap_null_distribution()
    test_normalize_scores_percentile()
    test_normalize_scores_zscore()
    print("All tests passed!")
```
